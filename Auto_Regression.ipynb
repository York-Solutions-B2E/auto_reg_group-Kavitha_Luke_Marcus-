{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries for data visualization\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import libraries for building linear regression model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Import library for preparing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import library for data preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "# Import libraries for scoring models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. import, X-y separation, train_test_split\n",
    "#### 2. EDA\n",
    "#### 3. create 6 datasets\n",
    "#### 4. impute, encoding, separate cat/num\n",
    "#### 5. build models\n",
    "#### 6. score models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"auto_1993_adj.csv\")\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop \"ID\" as it is unlikely to contribute to our analysis\n",
    "features = df.drop([\"ID\",\"mpg\"], axis=1)\n",
    "target = df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.hist(bins=25, figsize = (12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = df.corr()\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap=\"YlOrRd\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(data=df, corner=True, height=3)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.copy()\n",
    "# X = X.dropna(axis=0)\n",
    "# y = X.pop(\"mpg\")\n",
    "\n",
    "# # All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "# discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.copy()\n",
    "# X = X.dropna(axis=0)\n",
    "# y = X.pop(\"mpg\")\n",
    "\n",
    "# # All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "# discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(X.head())\n",
    "# # X.info()\n",
    "# y.head()\n",
    "\n",
    "\n",
    "# def make_mi_scores(X, y, discrete_features):\n",
    "#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "#     print(X.head())\n",
    "#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "#     mi_scores = mi_scores.sort_values(ascending=False)\n",
    "#     return mi_scores\n",
    "\n",
    "\n",
    "# X = data.copy()\n",
    "# X = X.dropna()\n",
    "# y = X.pop(\"mpg\")\n",
    "# print(y)\n",
    "# # print(X.head())\n",
    "# discrete_features = X.dtypes == int\n",
    "# mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "# print(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     mi_scores \u001b[38;5;241m=\u001b[39m mi_scores\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mi_scores\n\u001b[0;32m---> 11\u001b[0m mi_scores \u001b[38;5;241m=\u001b[39m make_mi_scores(\u001b[43mX\u001b[49m, y, discrete_features)\n\u001b[1;32m     12\u001b[0m mi_scores[::\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# show a few features with their MI scores\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    print(X.head())\n",
    "    print(y.shape)\n",
    "    print(discrete_features)\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features\n",
    "features[\"P_to_W\"] = features[\"horsepower\"] / features[\"weight\"]\n",
    "features[\"D_to_W\"] = features[\"displacement\"] / features[\"weight\"]\n",
    "features[\"SIZE\"] = features[\"displacement\"] + features[\"weight\"]\n",
    "features[\"Performance\"] = features[\"horsepower\"] + features[\"acceleration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 Distinct Feature Sets\n",
    "# # Set 1\n",
    "df1 = features[[\"displacement\",\"cylinders\",\"horsepower\",\"weight\",\"acceleration\",\"model_year\",\"origin\"]]\n",
    "df2 = features[[\"displacement\", \"horsepower\",\"weight\",\"acceleration\"]]  \n",
    "df3 = features[[\"cylinders\",\"origin\"]]                      \n",
    "df4 = features[[\"model_year\",\"weight\", \"cylinders\"]]            \n",
    "df5 = features[[\"displacement\", \"horsepower\"]]\n",
    "df6 = features[[\"Performance\"]]      \n",
    "df6.shape                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_regres(data, model):\n",
    "    # Separate features and target\n",
    "    X = data\n",
    "    global target\n",
    "    y = target\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=16\n",
    "    )\n",
    "\n",
    "\n",
    "    # Separate CAT & NUM features\n",
    "    cat_cols = [\"cylinders\", \"origin\"]\n",
    "    num_cols = [\"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"P_to_W\",\"D_to_W\",\"SIZE\",\"Performance\"]\n",
    "    cat_cols = [col for col in data.columns if col in cat_cols]\n",
    "    num_cols = [col for col in data.columns if col in num_cols]\n",
    "    # print(cat_cols)\n",
    "    # print(num_cols)\n",
    "\n",
    "    # Transform NUM Data\n",
    "    numerical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer()),\n",
    "            (\"scale\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Transform CAT Data\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # # Define models\n",
    "    # knn = KNeighborsRegressor()\n",
    "    # svr = SVR()\n",
    "    # ridge = Ridge()\n",
    "\n",
    "    # Preprocess data then create model\n",
    "    mod_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    # print(X_train.tail(5))\n",
    "    # print(X_train.shape)\n",
    "    # print(type(X_train))\n",
    "    # print(y_train.tail(5))\n",
    "    # print(y_train.shape)\n",
    "    # print(type(y_train))\n",
    "\n",
    "    # Preprocess training data, fit model\n",
    "    mod_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions for training data\n",
    "    preds = mod_pipeline.predict(X_train)\n",
    "\n",
    "    return \" MAE:\", mean_squared_error(y_train, preds), \"R_2:\", r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "knn = KNeighborsRegressor()\n",
    "svr = SVR()\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 (' MAE:', 5.870642953020134, 'R_2:', 0.8990686907003489)\n",
      "df2 (' MAE:', 10.70973154362416, 'R_2:', 0.8158724290344264)\n",
      "df3 (' MAE:', 21.628995973154364, 'R_2:', 0.6281424539224804)\n",
      "df4 (' MAE:', 5.46551543624161, 'R_2:', 0.9060338648778616)\n",
      "df5 (' MAE:', 10.899906040268457, 'R_2:', 0.8126028449198216)\n",
      "df6 (' MAE:', 14.597138255033558, 'R_2:', 0.7490380035204454)\n"
     ]
    }
   ],
   "source": [
    "print(\"df1\",auto_regres(df1, knn))\n",
    "print(\"df2\",auto_regres(df2, knn))\n",
    "print(\"df3\",auto_regres(df3, knn))\n",
    "print(\"df4\",auto_regres(df4, knn))\n",
    "print(\"df5\",auto_regres(df5, knn))\n",
    "print(\"df6\",auto_regres(df6, knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv_regres(data, model):\n",
    "#     # Separate features and target\n",
    "#     X = data.drop(\"mpg\", axis=1)\n",
    "#     y = data[\"mpg\"]\n",
    "\n",
    "#     # train test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.25, random_state=16\n",
    "#     )\n",
    "\n",
    "#     # Separate CAT & NUM features\n",
    "#     cat_cols = [\"cylinders\", \"origin\"]\n",
    "#     num_cols = [\"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "#     cat_cols = [col for col in data.columns if col in cat_cols]\n",
    "#     num_cols = [col for col in data.columns if col in num_cols]\n",
    "\n",
    "#     # Transform NUM Data\n",
    "#     numerical_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer()),\n",
    "#             (\"scale\", StandardScaler()),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Transform CAT Data\n",
    "#     categorical_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#             (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "#         ]\n",
    "#     )\n",
    "#     # Create Data Preprocessor Process\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", numerical_transformer, num_cols),\n",
    "#             (\"cat\", categorical_transformer, cat_cols),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Define models\n",
    "#     knn = KNeighborsRegressor()\n",
    "#     svr = SVR()\n",
    "#     ridge = Ridge()\n",
    "\n",
    "#     # Preprocess data then create model\n",
    "#     mod_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "#     # Cross-Validation Code\n",
    "#     scores = -1 * cross_val_score(\n",
    "#         mod_pipeline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n",
    "#     )\n",
    "\n",
    "#     print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_regres(df1,ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_imputer = SimpleImputer()\n",
    "# X_train_imputed = pd.DataFrame(simple_imputer.fit_transform(X_train))\n",
    "# X_test_imputed = pd.DataFrame(simple_imputer.transform(X_test))\n",
    "# X_train_imputed.columns = X_train.columns\n",
    "# X_test_imputed.columns = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scale\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply ordinal encoder to each column with categorical data\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "# label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One Hot Encoder\n",
    "# # Apply one-hot encoder to each column with categorical data\n",
    "# OHE = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "# OH_cols_train = pd.DataFrame(OHE.fit_transform(X_train_imputed[cat_cols]))\n",
    "# OH_cols_valid = pd.DataFrame(OHE.transform(X_test_imputed[cat_cols]))\n",
    "\n",
    "# # One-hot encoding removed index; put it back\n",
    "# OH_cols_train.index = X_train_imputed.index\n",
    "# OH_cols_valid.index = X_test_imputed.index\n",
    "\n",
    "# # Remove categorical columns (will replace with one-hot encoding)\n",
    "# num_X_train_imputed = X_train_imputed.drop(cat_cols, axis=1)\n",
    "# num_X_test_imputed = X_test_imputed.drop(cat_cols, axis=1)\n",
    "\n",
    "# # Add one-hot encoded columns to numerical features\n",
    "# OH_X_train_imputed = pd.concat([num_X_train_imputed, OH_cols_train], axis=1)\n",
    "# OH_X_test_imputed = pd.concat([num_X_test_imputed, OH_cols_valid], axis=1)\n",
    "\n",
    "# # Ensure all columns have string type\n",
    "# OH_X_train_imputed.columns = OH_X_train_imputed.columns.astype(str)\n",
    "# OH_X_test_imputed.columns = OH_X_test_imputed.columns.astype(str)\n",
    "\n",
    "# # print(\"MAE from Approach 3 (One-Hot Encoding):\")\n",
    "# # print(score_dataset(OH_X_train_imputed, OH_X_test_imputed, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OH_X_train_imputed.head(15)\n",
    "# OH_X_train_imputed.rename(\n",
    "#     columns={\n",
    "#         \"0\": \"cyl_3\",\n",
    "#         \"1\": \"cyl_4\",\n",
    "#         \"2\": \"cyl_5\",\n",
    "#         \"3\": \"cyl_6\",\n",
    "#         \"4\": \"cyl_8\",\n",
    "#         \"5\": \"org_1\",\n",
    "#         \"6\": \"org_2\",\n",
    "#         \"7\": \"org_3\"},\n",
    "#     inplace=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "knn = KNeighborsRegressor()\n",
    "svr = SVR()\n",
    "ridge = Ridge()\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", knn)])\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_train)\n",
    "\n",
    "# score = mean_absolute_error(y_valid, preds)\n",
    "print(\" MAE:\", mean_squared_error(y_train, preds))\n",
    "print(\"R_2:\", r2_score(y_train, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
